{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer 2 \n",
    "\n",
    "'''\n",
    "a. Build a machine learning model to predict customer churn based on a given dataset. Train the model using appropriate algorithms and\n",
    "evaluate its performance.\n",
    "'''\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the customer churn dataset\n",
    "data = pd.read_csv(\"customer_churn.csv\")\n",
    "\n",
    "# Split the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, data[\"Churn\"], test_size=0.25)\n",
    "\n",
    "# Train a logistic regression model\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model's performance\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "b. Develop a model training pipeline that incorporates feature engineering techniques such as one-hot encoding, feature scaling, and dimensionality reduction.\n",
    "'''\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, PCA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the customer churn dataset\n",
    "data = pd.read_csv(\"customer_churn.csv\")\n",
    "\n",
    "# One-hot encode the categorical features\n",
    "encoder = OneHotEncoder()\n",
    "X_cat = encoder.fit_transform(data[[\"Gender\", \"Tenure\", \"Contract\"]])\n",
    "\n",
    "# Scale the numerical features\n",
    "scaler = StandardScaler()\n",
    "X_num = scaler.fit_transform(data[[\"MonthlyCharges\", \"TotalCharges\"]])\n",
    "\n",
    "# Combine the categorical and numerical features\n",
    "X = np.concatenate([X_cat, X_num], axis=1)\n",
    "\n",
    "# Split the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, data[\"Churn\"], test_size=0.25)\n",
    "\n",
    "# Reduce the dimensionality of the data using PCA\n",
    "pca = PCA(n_components=10)\n",
    "X_train = pca.fit_transform(X_train)\n",
    "X_test = pca.transform(X_test)\n",
    "\n",
    "# Train a logistic regression model\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model's performance\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "c. Train a deep learning model for image classification using transfer learning and fine-tuning techniques.\n",
    "'''\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "\n",
    "# Load the VGG16 model\n",
    "vgg16 = VGG16(weights=\"imagenet\", include_top=False)\n",
    "\n",
    "# Freeze the VGG16 model\n",
    "for layer in vgg16.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Add a new dense layer on top of the VGG16 model\n",
    "model = Sequential()\n",
    "model.add(vgg16)\n",
    "model.add(Dense(1000, activation=\"relu\"))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(10, activation=\"softmax\"))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=10)\n",
    "\n",
    "# Evaluate the model's performance\n",
    "model.evaluate(X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer 1\n",
    "\n",
    "\n",
    "'''\n",
    "a. Design a data ingestion pipeline that collects and stores data from various sources such as databases, APIs, and streaming platforms.\n",
    "'''\n",
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# Define the API endpoint\n",
    "endpoint = \"https://api.example.com/data\"\n",
    "\n",
    "# Make the API request\n",
    "response = requests.get(endpoint)\n",
    "\n",
    "# Check the response status code\n",
    "if response.status_code == 200:\n",
    "    # The request was successful\n",
    "    data = json.loads(response.content)\n",
    "else:\n",
    "    # The request failed\n",
    "    print(\"Error:\", response.status_code)\n",
    "\n",
    "# Store the data in a Pandas DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv(\"data.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "b. Implement a real-time data ingestion pipeline for processing sensor data from IoT devices.\n",
    "'''\n",
    "import paho.mqtt.client as mqtt\n",
    "\n",
    "# Create a client\n",
    "client = mqtt.Client()\n",
    "\n",
    "# Connect to the broker\n",
    "client.connect(\"localhost\", 1883)\n",
    "\n",
    "# Subscribe to the topic\n",
    "client.subscribe(\"sensor/data\")\n",
    "\n",
    "# Define a callback function\n",
    "def on_message(client, userdata, message):\n",
    "    print(message.payload.decode())\n",
    "\n",
    "# Register the callback function\n",
    "client.on_message = on_message\n",
    "\n",
    "# Start listening for messages\n",
    "client.loop_forever()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "c. Develop a data ingestion pipeline that handles data from different file formats (CSV, JSON, etc.) and performs data validation and cleansing.\n",
    "\n",
    "'''\n",
    "import csv\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# Define the file path\n",
    "filepath = \"data.csv\"\n",
    "\n",
    "# Read the data from the file\n",
    "with open(filepath, \"r\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    data = list(reader)\n",
    "\n",
    "# Validate the data\n",
    "for row in data:\n",
    "    if len(row) != 3:\n",
    "        raise ValueError(\"Invalid row:\", row)\n",
    "\n",
    "# Cleanse the data\n",
    "for row in data:\n",
    "    for i in range(3):\n",
    "        row[i] = row[i].strip()\n",
    "\n",
    "# Store the data in a Pandas DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Save the DataFrame to a JSON file\n",
    "df.to_json(\"data.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# answer 3\n",
    "\n",
    "'''\n",
    "a. Implement cross-validation to evaluate the performance of a regression model for predicting housing prices.\n",
    "\n",
    "'''\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Load the housing prices dataset\n",
    "data = np.loadtxt(\"housing_prices.csv\", delimiter=\",\")\n",
    "\n",
    "# Split the data into features and labels\n",
    "X = data[:, :-1]\n",
    "y = data[:, -1]\n",
    "\n",
    "# Create a linear regression model\n",
    "model = LinearRegression()\n",
    "\n",
    "# Perform 10-fold cross-validation\n",
    "scores = cross_val_score(model, X, y, cv=10)\n",
    "\n",
    "# Print the cross-validation scores\n",
    "print(scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "b. Perform model validation using different evaluation metrics such as accuracy, precision, recall, and F1 score for a binary classification problem.\n",
    "\n",
    "'''\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Load the binary classification dataset\n",
    "data = np.loadtxt(\"binary_classification.csv\", delimiter=\",\")\n",
    "\n",
    "# Split the data into features and labels\n",
    "X = data[:, :-1]\n",
    "y = data[:, -1]\n",
    "\n",
    "# Split the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)\n",
    "\n",
    "# Train a logistic regression model\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy, precision, recall, and F1 score\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 score:\", f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "c. Design a model validation strategy that incorporates stratified sampling to handle imbalanced datasets.\n",
    "\n",
    "'''\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the imbalanced dataset\n",
    "data = np.loadtxt(\"imbalanced_dataset.csv\", delimiter=\",\")\n",
    "\n",
    "# Count the number of samples for each class\n",
    "class_counts = np.unique(data[:, -1], return_counts=True)\n",
    "\n",
    "# Calculate the class imbalance ratio\n",
    "imbalance_ratio = class_counts[1] / class_counts[0]\n",
    "\n",
    "# Perform stratified sampling to balance the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, data[:, -1], test_size=0.25, stratify=data[:, -1])\n",
    "\n",
    "# Train a logistic regression model\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Print the accuracy\n",
    "print(\"Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# answer 4\n",
    "'''\n",
    "a. Create a deployment strategy for a machine learning model that provides real-time recommendations based on user interactions.\n",
    "\n",
    "'''\n",
    "import boto3\n",
    "import json\n",
    "\n",
    "# Define the AWS Lambda function\n",
    "def recommender(event, context):\n",
    "    # Get the user's interactions\n",
    "    interactions = json.loads(event[\"interactions\"])\n",
    "\n",
    "    # Make recommendations\n",
    "    recommendations = []\n",
    "    for interaction in interactions:\n",
    "        product = interaction[\"product\"]\n",
    "        rating = interaction[\"rating\"]\n",
    "        recommendations.append(product)\n",
    "\n",
    "    # Return the recommendations\n",
    "    return {\"recommendations\": recommendations}\n",
    "\n",
    "# Deploy the Lambda function\n",
    "lambda_client = boto3.client(\"lambda\")\n",
    "lambda_client.create_function(\n",
    "    FunctionName=\"recommender\",\n",
    "    Runtime=\"python3.8\",\n",
    "    Handler=\"recommender.lambda_handler\",\n",
    "    Code=open(\"recommender.py\", \"rb\").read(),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "b. Develop a deployment pipeline that automates the process of deploying machine learning models to cloud platforms such as AWS or Azure.\n",
    "\n",
    "'''\n",
    "import argparse\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "def deploy_model(model_path, cloud_platform):\n",
    "    if cloud_platform == \"aws\":\n",
    "        subprocess.run([\"aws\", \"lambda\", \"deploy\", \"-f\", model_path, \"-t\", \"python3.8\"])\n",
    "    elif cloud_platform == \"azure\":\n",
    "        subprocess.run([\"az\", \"ml\", \"model\", \"deploy\", \"-f\", model_path])\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--model_path\", required=True)\n",
    "    parser.add_argument(\"--cloud_platform\", choices=[\"aws\", \"azure\"], required=True)\n",
    "    args = parser.parse_args()\n",
    "    deploy_model(args.model_path, args.cloud_platform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "c. Design a monitoring and maintenance strategy for deployed models to ensure their performance and reliability over time.\n",
    "\n",
    "'''\n",
    "import boto3\n",
    "import json\n",
    "\n",
    "# Define the AWS CloudWatch metrics\n",
    "metrics = [\n",
    "    \"Latency\",\n",
    "    \"Errors\",\n",
    "    \"Invocations\",\n",
    "]\n",
    "\n",
    "# Define the AWS CloudWatch alarms\n",
    "alarms = [\n",
    "    {\n",
    "        \"MetricName\": \"Latency\",\n",
    "        \"Threshold\": 100,\n",
    "        \"ComparisonOperator\": \"GreaterThanThreshold\",\n",
    "    },\n",
    "    {\n",
    "        \"MetricName\": \"Errors\",\n",
    "        \"Threshold\": 10,\n",
    "        \"ComparisonOperator\": \"GreaterThanThreshold\",\n",
    "    },\n",
    "]\n",
    "\n",
    "# Create the CloudWatch alarms\n",
    "cloudwatch_client = boto3.client(\"cloudwatch\")\n",
    "for alarm in alarms:\n",
    "    cloudwatch_client.create_alarm(\n",
    "        MetricName=alarm[\"MetricName\"],\n",
    "        Threshold=alarm[\"Threshold\"],\n",
    "        ComparisonOperator=alarm[\"ComparisonOperator\"],\n",
    "    )\n",
    "\n",
    "# Monitor the model\n",
    "while True:\n",
    "    # Get the metrics from CloudWatch\n",
    "    metrics_data = cloudwatch_client.get_metric_data(\n",
    "        MetricNames=metrics,\n",
    "        StartTime=datetime.now() - timedelta(days=1),\n",
    "        EndTime=datetime.now(),\n",
    "    )\n",
    "\n",
    "    # Check the alarms\n",
    "    for alarm in alarms:\n",
    "        if metrics_data[alarm[\"MetricName\"]][\"Datapoints\"][0][\"Value\"] > alarm[\"Threshold\"]:\n",
    "            print(\"Alarm triggered:\", alarm[\"MetricName\"])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
